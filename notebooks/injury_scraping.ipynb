{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cdcb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48befec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_map = {'Monday': 0, 'Tuesday': 1, 'Wednesday': 2, 'Thursday': 3, 'Friday': 4, 'Saturday': 5, 'Sunday': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6505d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weekdays_between(start, end, day_of_week='Thursday'):\n",
    "    \"\"\"Generate all Thursdays between two dates.\"\"\"\n",
    "    days = []\n",
    "    current = start\n",
    "    while current <= end:\n",
    "        if current.weekday() == day_map[day_of_week]:\n",
    "            days = days + [current]\n",
    "        current += timedelta(days=1)\n",
    "    return days\n",
    "\n",
    "def get_gameweek_number(urls):\n",
    "    \"\"\"Extract gameweek number from URLs.\"\"\"\n",
    "    gameweek_gets = []\n",
    "    for url in urls:\n",
    "        if 'gameweek' in url:\n",
    "            parts = url.split('-')\n",
    "            for i in range(len(parts)):\n",
    "                if parts[i].startswith('gameweek'):\n",
    "                    gameweek_gets = gameweek_gets + [parts[i+1]]\n",
    "\n",
    "    # check if we have any gameweek numbers and that there is one unique value\n",
    "    if gameweek_gets:\n",
    "        unique_gws = set(gameweek_gets)\n",
    "        if len(unique_gws) == 1:\n",
    "            return int(unique_gws.pop())\n",
    "        else:\n",
    "            print(f\"Multiple gameweek numbers found: {unique_gws}\")\n",
    "            return \"uncertain\"\n",
    "    else:\n",
    "        print(\"No gameweek number found in URLs.\")\n",
    "        print(urls)\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def list_articles_for_date(date: datetime):\n",
    "    base_url = f\"https://www.fantasyfootballscout.co.uk/{date.year}/{date.strftime('%m')}/{date.strftime('%d')}/\"\n",
    "    try:\n",
    "        res = requests.get(base_url, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        # Find all anchor tags with hrefs\n",
    "        links = soup.find_all(\"a\", href=True)\n",
    "        urls = set()\n",
    "\n",
    "        for link in links:\n",
    "            href = link[\"href\"]\n",
    "            if href.startswith(base_url):\n",
    "                urls.add(href)\n",
    "\n",
    "        # filter to urls containing 'injury'\n",
    "        # urls = [url for url in urls if 'injury' in url or 'team-news' in url or 'team-updates' in url or 'fpl' in url]\n",
    "        urls = [url for url in urls if 'gameweek' in url or 'fpl' in url]\n",
    "        urls = [url for url in urls if 'comments' not in url]\n",
    "\n",
    "        return sorted(urls)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch or parse {base_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_article(url):\n",
    "    try:\n",
    "        res = requests.get(url, timeout=10)\n",
    "        res.raise_for_status()\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        content_div = soup.find(\"article\")\n",
    "        if not content_div:\n",
    "            return None\n",
    "        paragraphs = content_div.find_all(\"p\")\n",
    "        text = \"\\n\\n\".join(p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True))\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to scrape {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_season(date):\n",
    "    year = int(date.strftime('%y'))\n",
    "    month = int(date.strftime('%m'))\n",
    "    if month < 8:\n",
    "        return f\"{year - 1}-{year}\"\n",
    "    else:\n",
    "        return f\"{year}-{year + 1}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0877bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = []\n",
    "missing_date_urls = []\n",
    "\n",
    "# for each year\n",
    "for season_year in [2024]:\n",
    "    print(season_year)\n",
    "    season_end = datetime(season_year + 1, 5, 31)\n",
    "    season_start = datetime(season_year, 8, 1)\n",
    "\n",
    "    thursdays = get_weekdays_between(season_start, season_end)\n",
    "    saturdays = get_weekdays_between(season_start, season_end, 'Saturday')\n",
    "    update_dates = thursdays + saturdays\n",
    "\n",
    "\n",
    "    for date in tqdm(update_dates[0:5]):\n",
    "\n",
    "        urls = list_articles_for_date(date)\n",
    "        day_of_week = date.strftime('%A')\n",
    "\n",
    "        season = get_season(date)\n",
    "\n",
    "        if len(urls) > 0:\n",
    "            gameweek = get_gameweek_number(urls)\n",
    "            for url in urls:\n",
    "                content = scrape_article(url)\n",
    "                if content:\n",
    "                    articles.append({\n",
    "                        \"date\": date.strftime(\"%Y-%m-%d\"),\n",
    "                        \"gameweek\": gameweek,\n",
    "                        \"season\": season,\n",
    "                        \"day_of_week\": day_of_week,\n",
    "                        \"url\": url,\n",
    "                        \"content\": content\n",
    "                    })\n",
    "        else:\n",
    "            missing_date_urls = missing_date_urls + [date.strftime('%Y-%m-%d')]\n",
    "\n",
    "print(f\"Missing URLs for dates: \")\n",
    "for d in missing_date_urls:\n",
    "    print(f\"- {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68671d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff066bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(missing_date_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54763496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by date\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values(by='date')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ac46ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['day_of_week'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d649e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import subprocess\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796efc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_ollama(prompt, model=\"llama3\"):\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [\"ollama\", \"run\", model],\n",
    "            input=prompt,\n",
    "            text=True,\n",
    "            capture_output=True,\n",
    "            check=True,\n",
    "        )\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Ollama error: {e.stderr}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_injuries_with_summary_ollama(content):\n",
    "    prompt = f\"\"\"\n",
    "You are a helpful assistant reading a sports news article. Ignoring the comments, your task is to:\n",
    "\n",
    "1. Summarise the article in 2â€“3 sentences.\n",
    "2. Extract the names of any players mentioned as potentially injured/suspended/missing and assess the likelihood they will miss the game (as certain/high/low/unsure).\n",
    "3. Extract the names of players who definitely will be playing\n",
    "\n",
    "Return your answer as a JSON with the fields:\n",
    "- \"summary\": <string>\n",
    "- \"missing_players\": <list of dicts with keys \"player\", \"team\", \"reason\", and \"absence_likelihood\">\n",
    "- \"playing_players\": <list of dicts with keys \"player\", \"team\">\n",
    "\n",
    "Here is the article content:\n",
    "\\\"\\\"\\\"\n",
    "{content}\n",
    "\\\"\\\"\\\"\n",
    "\"\"\"\n",
    "    response = query_ollama(prompt)\n",
    "\n",
    "    # Attempt to extract the first JSON-like block using regex\n",
    "    try:\n",
    "        json_match = re.search(r\"\\{.*\\}\", response, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_text = json_match.group()\n",
    "        else:\n",
    "            raise ValueError(\"No JSON object found in response\")\n",
    "\n",
    "        # Try parsing with standard json\n",
    "        try:\n",
    "            parsed = json.loads(json_text)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fallback to safe evaluation\n",
    "            parsed = ast.literal_eval(json_text)\n",
    "\n",
    "\n",
    "        return parsed.get(\"summary\", \"\"), parsed.get(\"missing_players\", []), parsed.get(\"playing_players\", [])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Parsing error: {e}\\nRaw response:\\n{response}\")\n",
    "        return \"\", [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44b3824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a3e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['season'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa1168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# substitute to 2024 and between game week 20 to 30\n",
    "df_sub = df[df['season'] == '24-25']\n",
    "df_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb20e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = []\n",
    "missing = []\n",
    "playing = []\n",
    "\n",
    "for _, row in tqdm(df_sub.iterrows(), total=len(df_sub)):\n",
    "    summary, absent, confirmed = extract_injuries_with_summary_ollama(row['content'])\n",
    "\n",
    "    # sort absent by team name\n",
    "    # absent = sorted(absent, key=lambda x: x.get('team', ''))\n",
    "\n",
    "    summaries.append(summary)\n",
    "    missing.append(absent)\n",
    "    playing.append(confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d561ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['summary'] = summaries\n",
    "df_sub['missing_players'] = missing\n",
    "df_sub['confirmed_players'] = playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d36e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336fbae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12431d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use = df_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2416f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use = df_use[df_use['missing_players'].apply(lambda x: isinstance(x, list))]\n",
    "df_use = df_use[df_use['confirmed_players'].apply(lambda x: isinstance(x, list))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0d0771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop where missing_players, players is ''\n",
    "df_use = df_use[df_use['missing_players'].apply(lambda x: x['players'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50f0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to where a given player is mentioned\n",
    "def player_subset(df, filter_name):\n",
    "    \"\"\"Subset DataFrame to rows where player_name is mentioned in missing or confirmed players.\"\"\"\n",
    "\n",
    "    player_missing = df[df['missing_players'].apply(lambda x: any(filter_name in player['player'] for player in x))]\n",
    "    # player_confirmed = df[df['confirmed_players'].apply(lambda x: any(filter_name.lower() in player['player'].lower() for player in x))]\n",
    "\n",
    "    player_mentions = pd.concat([player_missing], ignore_index=True)\n",
    "\n",
    "    # subset the missing_players to only those mentioning Ben White\n",
    "    player_mentions['missing_players'] = player_mentions['missing_players'].apply(\n",
    "        lambda x: [player for player in x if filter_name in player['player']]\n",
    "    )\n",
    "    player_mentions['confirmed_players'] = player_mentions['confirmed_players'].apply(\n",
    "        lambda x: [player for player in x if filter_name in player['player']]\n",
    "    )\n",
    "\n",
    "    return player_mentions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d5e8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use.loc[1, 'missing_players']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac17cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e617e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use['missing_players']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6e3383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_use['missing_players']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e44240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ben white\n",
    "# 25-30 2024\n",
    "# arsenal\n",
    "\n",
    "df_haaland = player_subset(df_use, 'Haaland')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26be290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_haaland"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66790b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in df_haaland.iterrows():\n",
    "    print(row['missing_players'])\n",
    "    print(row['confirmed_players'])\n",
    "    print(row['url'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d35e752",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airsenalenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
